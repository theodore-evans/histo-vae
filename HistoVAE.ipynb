{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling histological FOV data from BreastIHC API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image and API responses are extracted from MindPeak's [BreastIHC](https://ihc.mindpeak.ai/) app (free registration required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import figure\n",
    "import numpy as np\n",
    "import PIL\n",
    "import math\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responses and associated image from the Ki67 demo images - we can gather more data using uploaded breast cell image data (e.g. from [CAMELYON](http://gigadb.org/dataset/100439))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSES = [\n",
    "    \"labC_1.bd7b589b\",\n",
    "    \"labB_1.fbc70936\",\n",
    "    \"labB_0.0456cf3f\",\n",
    "    \"labA_0.722f7db3\"\n",
    "]\n",
    "\n",
    "RESPONSES_DIR = \"Responses/\"\n",
    "IMAGES_DIR = \"Images/\"\n",
    "\n",
    "CROP_WIDTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parses the API response json and adds the associated image data 'thumbnails', both as np.array and a PIL, for each classified cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_api_response(responses, index):\n",
    "    response_path = RESPONSES_DIR + \"response_\" + responses[index] + \".json\"\n",
    "    with open(response_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        cell_dict = data[\"response\"][\"results\"][\"cells\"]\n",
    "        df = pd.DataFrame.from_dict(cell_dict)\n",
    "        \n",
    "    image_path = IMAGES_DIR + responses[index] + \".png\"\n",
    "    img = plt.imread(image_path)\n",
    "    \n",
    "    add_cell_images_to_df(df, img,  CROP_WIDTH)\n",
    "    return df\n",
    "\n",
    "def add_cell_images_to_df(df, img, crop_width):\n",
    "    image_data = []\n",
    "    images = []\n",
    "    for index, row in df.iterrows():\n",
    "        center = (df.x[index] + 64, df.y[index] + 64) # center of the region\n",
    "        crop_offset = math.floor(crop_width/2)\n",
    "        cropped = img[(center[1] - crop_offset):(center[1] + crop_offset),\n",
    "                   (center[0] - crop_offset):(center[0] + crop_offset),:]\n",
    "\n",
    "        image_data.append(cropped)\n",
    "        images.append(PIL.Image.fromarray(np.uint8(cropped*255)))\n",
    "            \n",
    "    df[\"image_data\"] = image_data\n",
    "    df[\"image\"] = images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reclassifies cell type based on 'intensity' data, I guess that this should be equivalent to moving the Positivity Threshold slider in the BreastIHC GUI, but the numbers don't seem to match up perfectly. Either there is some logic that I'm missing, or it's a bug!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reclassify_by_intensity(df, intensity_threshold):\n",
    "    df.loc[df.intensity < intensity_threshold, \"type\"] = \"non_tumor\"\n",
    "    df.loc[df.intensity >= intensity_threshold, \"type\"] = \"tumor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproduces the BreastIHC GUI for a given api response. There is a 64 pixel margin around the FOV where there is no classification, assumedly to omit any classifications affected by border effects of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_api_response(responses, index, intensity_threshold):\n",
    "\n",
    "    df = parse_api_response(RESPONSES, 0)\n",
    "    reclassify_by_intensity(df, intensity_threshold)\n",
    "\n",
    "    grouped_by_type = df.groupby(\"type\")\n",
    "    tumor_cells = grouped_by_type.get_group(\"tumor\")\n",
    "    non_tumor_cells = grouped_by_type.get_group(\"non_tumor\")\n",
    "    \n",
    "    image_path = IMAGES_DIR + responses[index] + \".png\"\n",
    "    img = plt.imread(image_path)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    point_size = 10\n",
    "    plt.scatter(tumor_cells.x + 64, tumor_cells.y + 64, s = point_size, c = 'red')\n",
    "    plt.scatter(non_tumor_cells.x + 64, non_tumor_cells.y + 64, s = point_size, c = 'green')\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(18, 10 , forward=True)\n",
    "    plt.show()\n",
    "    print(\"tumor cells: \" + str(len(tumor_cells.index))+ \"\\nnon-tumor cells: \" + str(len(non_tumor_cells.index))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4c5d0d8b27a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_api_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESPONSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintensity_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-a09f9cb5197b>\u001b[0m in \u001b[0;36mdisplay_api_response\u001b[0;34m(responses, index, intensity_threshold)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_api_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintensity_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_api_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESPONSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mreclassify_by_intensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintensity_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c144e0c1997a>\u001b[0m in \u001b[0;36mparse_api_response\u001b[0;34m(responses, index)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0madd_cell_images_to_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mCROP_WIDTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c144e0c1997a>\u001b[0m in \u001b[0;36madd_cell_images_to_df\u001b[0;34m(df, img, crop_width)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcrop_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_width\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         cropped = img[(center[1] - crop_width/2):(center[1] + crop_width/2),\n\u001b[0;32m---> 21\u001b[0;31m                    (center[0] - crop_width/2):(center[0] + crop_width/2),:]\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimage_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcropped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "display_api_response(RESPONSES, index = 0, intensity_threshold = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a number of random cell images from the parsed dataframe, along with its classification for some intensity threshold (to check that it's working!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_threshold = 0.5\n",
    "number_of_images_to_display = 1\n",
    "cell_df = parse_api_response(RESPONSES, 0)\n",
    "reclassify_by_intensity(cell_df, intensity_threshold)\n",
    "\n",
    "for i in range(number_of_images_to_display):\n",
    "    random_cell_index = np.random.randint(0, len(cell_df.index))\n",
    "\n",
    "    plt.imshow(cell_df[\"image_data\"][random_cell_index])\n",
    "    plt.show()\n",
    "    print(cell_df[\"type\"][random_cell_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CV)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
